import { Code } from 'lucide-react';

export const examplesContent = {
    themes: [

        {
            id: 'simulation',
            title: 'Simulation',
            description: 'Mod√©lisation et M√©thodes Num√©riques',
            categories: [
                {
                    id: 'ode',
                    title: '√âquations Diff√©rentielles',
                    description: 'R√©solution num√©rique d\'EDO.',
                    snippets: [
                        {
                            id: 'euler',
                            title: 'M√©thode d\'Euler (Ordre 1)',
                            description: 'La m√©thode la plus simple et intuitive.',
                            code: `import numpy as np
import matplotlib.pyplot as plt

def euler_method(f, y0, t0, tf, h):
    """
    R√©sout y' = f(t, y) avec la m√©thode d'Euler.
    
    --- üß† INTUITION ---
    C'est l'approche la plus na√Øve : on regarde la pente au point actuel
    et on trace une ligne droite jusqu'au point suivant.
    
    Formule : y_next = y + h * pente
    
    --- ‚ö†Ô∏è LIMITES ---
    Comme on suppose que la pente est constante sur tout l'intervalle h,
    l'erreur s'accumule vite. C'est une m√©thode d'ordre 1 (erreur proportionnelle √† h).
    """
    t_values = np.arange(t0, tf + h, h)
    y_values = [y0]

    for t in t_values[:-1]:
        y = y_values[-1]
        
        # On avance tout droit en suivant la pente actuelle
        y_next = y + h * f(t, y)
        
        y_values.append(y_next)

    return t_values, np.array(y_values)`
                        },
                        {
                            id: 'rk2',
                            title: 'Runge-Kutta 2 (RK2 / Point Milieu)',
                            description: 'Plus pr√©cis qu\'Euler, moins co√ªteux que RK4.',
                            code: `def runge_kutta_2(f, y0, t0, tf, h):
    """
    R√©sout y' = f(t, y) avec la m√©thode RK2 (Point Milieu).
    
    --- üß† INTUITION ---
    Euler se trompe car la pente change pendant le trajet.
    RK2 essaie d'anticiper ce changement.
    
    1. On fait un demi-pas avec Euler pour estimer la pente au milieu.
    2. On utilise cette pente du milieu pour faire le vrai pas entier.
    
    --- ‚öóÔ∏è FORMULE ---
    k1 = h * f(t, y)          -> Pente au d√©but
    k2 = h * f(t + h, y + k1) -> Pente estim√©e √† la fin (ou milieu selon variante)
    
    y_next = y + 0.5 * (k1 + k2) -> Moyenne des deux pentes
    """
    t_values = np.arange(t0, tf + h, h)
    y_values = [y0]

    for t in t_values[:-1]:
        y = y_values[-1]
        
        # 1. Pente au d√©but
        k1 = h * f(t, y)
        
        # 2. Pente √† la fin (estim√©e avec k1)
        k2 = h * f(t + h, y + k1)
        
        # Moyenne des deux pentes (M√©thode de Heun / Trap√®ze)
        y_next = y + 0.5 * (k1 + k2)
        
        y_values.append(y_next)

    return t_values, np.array(y_values)`
                        },
                        {
                            id: 'rk4',
                            title: 'Runge-Kutta 4 (RK4)',
                            description: 'M√©thode standard pour r√©soudre les √©quations diff√©rentielles.',
                            code: `import numpy as np
import matplotlib.pyplot as plt

def runge_kutta_4(f, y0, t0, tf, h):
    """
    R√©sout l'√©quation diff√©rentielle y' = f(t, y) avec la m√©thode RK4.
    
    --- üß† INTUITION (Comment √ßa marche ?) ---
    Contrairement √† la m√©thode d'Euler qui suit b√™tement la pente du d√©but,
    RK4 est "intelligente" : elle t√¢te le terrain √† 4 endroits pour d√©cider o√π aller.
    
    --- üîç LES 4 PENTES (k1 √† k4) ---
    k1 : Pente au D√âBUT de l'intervalle.
         -> C'est la pr√©diction basique (comme Euler).
         
    k2 : Pente au MILIEU (estimation 1).
         -> On avance de h/2 avec la pente k1, et on regarde la pente l√†-bas.
         
    k3 : Pente au MILIEU (estimation 2).
         -> On refait une estimation au milieu, mais en utilisant k2 (correction).
         
    k4 : Pente √† la FIN.
         -> On utilise k3 pour estimer la pente tout √† la fin du pas h.
         
    --- ‚öóÔ∏è LA FORMULE MAGIQUE ---
    On fait une MOYENNE POND√âR√âE de ces 4 pentes :
    y_next = y + (h / 6) * (k1 + 2*k2 + 2*k3 + k4)
    
    Notez que les pentes du milieu (k2 et k3) comptent DOUBLE car elles sont
    g√©n√©ralement plus repr√©sentatives de la dynamique sur l'intervalle.
    
    --- üìù ARGUMENTS ---
    f  : La fonction d√©riv√©e (la physique du syst√®me). y' = f(t, y)
    y0 : √âtat initial (ex: position de d√©part).
    t0 : Temps de d√©but.
    tf : Temps de fin.
    h  : Pas de temps (plus il est petit, plus c'est pr√©cis).
    """
    t_values = np.arange(t0, tf + h, h)
    y_values = [y0]
    y = y0
    
    for t in t_values[:-1]:
        # 1. Pente au d√©but
        k1 = h * f(t, y)
        
        # 2. Pente au milieu (avec k1)
        k2 = h * f(t + 0.5 * h, y + 0.5 * k1)
        
        # 3. Pente au milieu (avec k2)
        k3 = h * f(t + 0.5 * h, y + 0.5 * k2)
        
        # 4. Pente √† la fin (avec k3)
        k4 = h * f(t + h, y + k3)
        
        # Moyenne pond√©r√©e
        y = y + (k1 + 2 * k2 + 2 * k3 + k4) / 6
        y_values.append(y)
        
    return t_values, np.array(y_values)

# --- Exemple : Croissance Exponentielle ---
# dy/dt = r * y
def modele_croissance(t, y):
    return 0.1 * y

# Simulation
t, y = runge_kutta_4(modele_croissance, y0=100, t0=0, tf=50, h=0.1)

# Plot
plt.plot(t, y, label='RK4')
plt.plot(t, 100 * np.exp(0.1 * t), '--', label='Exact')
plt.legend(); plt.show()`
                        }
                    ]
                }
            ]
        },
        {
            id: 'optimisation',
            title: 'Optimisation',
            description: 'Recherche Op√©rationnelle avec GurobiPy',
            categories: [
                {
                    id: 'gurobi',
                    title: 'GurobiPy',
                    description: 'Solveur d\'optimisation lin√©aire et mixte.',
                    snippets: [
                        {
                            id: 'knapsack',
                            title: 'Probl√®me du Sac √† Dos (Knapsack)',
                            description: 'Maximiser la valeur des objets dans un sac de capacit√© limit√©e.',
                            code: `import numpy as np
import gurobipy as gp
from gurobipy import GRB

def generate_knapsack(num_items):
    # Fix seed value
    rng = np.random.default_rng(seed=0)
    # Item values, weights
    values = rng.uniform(low=1, high=25, size=num_items)
    weights = rng.uniform(low=5, high=100, size=num_items)
    # Knapsack capacity
    capacity = 0.7 * weights.sum()

    return values, weights, capacity


def solve_knapsack_model(values, weights, capacity):
    num_items = len(values)
    # Turn values and weights numpy arrays to dict
    items = range(num_items)
    val = {i: float(values[i]) for i in items}
    wgt = {i: float(weights[i]) for i in items}

    with gp.Env() as env:
        with gp.Model(name="knapsack", env=env) as model:
            # Define decision variables using the Model.addVars() method
            x = model.addVars(items, vtype=GRB.BINARY, name="x")

            # Define objective function using the Model.setObjective() method
            # Build the LinExpr using the tupledict.prod() method
            model.setObjective(x.prod(val) , GRB.MAXIMIZE) # Maximise la somme des valeurs des objets choisis

            # Define capacity constraint using the Model.addConstr() method
            model.addConstr(x.prod(wgt) <= capacity, name="capacity") # La somme des poids <= capacit√©
            model.optimize()

data = generate_knapsack(10000)
solve_knapsack_model(*data)`
                        },
                        {
                            id: 'lot_sizing',
                            title: 'Lot Sizing (Planification de Production)',
                            description: 'Minimiser les co√ªts de production, stock et setup.',
                            code: `import json
import gurobipy as gp
from gurobipy import GRB
from pathlib import Path

# Note: Les donn√©es sont disponibles sur GitHub M2_Optimisation_Algorithmes_et_Data

# ----- Load data from JSON -----
# with open("data/data/lot_sizing_data.json", "r") as f:
#     data = json.load(f)

# Exemple de donn√©es pour que le code soit ex√©cutable
data = {
    "name": "LotSizing", "H": 5, "Qmin": 0, "Qmax": 100, "I0": 0,
    "demand": [10, 20, 15, 30, 10],
    "var_cost": [2, 2, 2, 3, 3],
    "setup_cost": [50, 50, 50, 50, 50],
    "hold_cost": [1, 1, 1, 1, 1]
}

name = data["name"]
H    = int(data["H"])
d    = [float(val) for val in data["demand"]]
c    = [float(val) for val in data["var_cost"]]
f    = [float(val) for val in data["setup_cost"]]
h    = [float(val) for val in data["hold_cost"]]
Qmin = float(data["Qmin"])
Qmax = float(data["Qmax"])
I0   = float(data["I0"])

# Basic validation
assert len(d) == H and len(c) == H and len(f) == H and len(h) == H
assert 0 <= Qmin <= Qmax

# ----- Build model -----
with gp.Env() as env, gp.Model(name, env=env) as model:
    x = model.addVars(H, lb=0.0, vtype=GRB.INTEGER, name="x") # Quantit√© produite
    y = model.addVars(H, vtype=GRB.BINARY, name="y")          # Setup (Produit ou pas ?)
    I = model.addVars(H, lb=0.0, vtype=GRB.INTEGER, name="I") # Stock

    # Objectif : Minimiser Co√ªt Var + Co√ªt Fixe + Co√ªt Stock
    model.setObjective(
        gp.quicksum(x[t]*c[t] + f[t]*y[t] + h[t]*I[t] for t in range(H)),
        GRB.MINIMIZE
    )

    # Constraints
    # √âquilibre des flux (P√©riode 0)
    model.addConstr(I[0] == I0 + x[0] - d[0], name="balance_0")

    # √âquilibre des flux (P√©riodes suivantes)
    model.addConstrs(
        (I[t] == I[t-1] + x[t] - d[t] for t in range(1, H)),
        name="Inventory_Balance"
    ) 

    # Capacit√© Max (Big M)
    model.addConstrs(
        (x[t] <= Qmax*y[t] for t in range(H)),
        name="Max_Production"
    )

    # Capacit√© Min
    model.addConstrs(
        (x[t] >= Qmin*y[t] for t in range(H)),
        name="Min_Production"
    )

    # Optimize
    model.optimize()

    if model.SolCount:
        print(f"Total cost = {model.ObjVal:.2f}")
        for t in range(H):
            print(f"t={t:2d}: y={int(y[t].X)} x={x[t].X:.1f} I={I[t].X:.1f}")`
                        }
                    ]
                }
            ]
        },
        {
            id: 'data_science',
            title: 'Data Science',
            description: 'Projets complets de Machine Learning',
            categories: [
                {
                    id: 'shoppers_intention',
                    title: 'Projet Complet (Shoppers Intention)',
                    description: 'Pr√©diction de l\'intention d\'achat (Classification)',
                    snippets: [
                        {
                            id: 'eda',
                            title: '1. Exploration des Donn√©es (EDA)',
                            description: 'Chargement, analyse de la target et corr√©lations. [T√©l√©charger le dataset](/MemoCode/data/online_shoppers_intention.csv)',
                            code: `import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Chargement des donn√©es
# Assurez-vous d'avoir le fichier 'online_shoppers_intention.csv'
df = pd.read_csv('online_shoppers_intention.csv')

print("Dimensions du dataset :", df.shape)
print(df.head())

# Distribution de la target 'Revenue'
plt.figure(figsize=(6, 4))
sns.countplot(x='Revenue', data=df)
plt.title('Distribution de la Target (Revenue)')
plt.show()

# Matrice de corr√©lation
plt.figure(figsize=(12, 10))
sns.heatmap(df.corr(numeric_only=True), annot=False, cmap='coolwarm')
plt.title('Matrice de Corr√©lation')
plt.show()`
                        },
                        {
                            id: 'preprocessing',
                            title: '2. Preprocessing & Feature Engineering',
                            description: 'Encodage, gestion des valeurs manquantes et SMOTE.',
                            code: `from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from imblearn.over_sampling import SMOTE

# 1. Gestion des valeurs manquantes (si n√©cessaire)
# Remplacement des NaN par la m√©diane pour les colonnes num√©riques
numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns
imputer = SimpleImputer(strategy='median')
df[numeric_cols] = imputer.fit_transform(df[numeric_cols])

# 2. Encodage des variables cat√©gorielles
le = LabelEncoder()
categorical_cols = df.select_dtypes(include=['object', 'bool']).columns

for col in categorical_cols:
    df[col] = le.fit_transform(df[col])

# 3. S√©paration Features (X) / Target (y)
X = df.drop('Revenue', axis=1)
y = df['Revenue']

# 4. Scaling (Standardisation)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 5. Gestion du d√©s√©quilibre de classe (SMOTE)
# La classe 'Revenue=True' est souvent minoritaire
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_scaled, y)

print(f"Taille originale : {X.shape}, Taille apr√®s SMOTE : {X_resampled.shape}")`
                        },
                        {
                            id: 'modeling',
                            title: '3. Mod√©lisation & √âvaluation',
                            description: 'Entra√Ænement d\'un mod√®le et analyse des performances.',
                            code: `from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Split Train / Test
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.2, random_state = 42)

# Entra√Ænement(Random Forest)
model = RandomForestClassifier(n_estimators = 100, random_state = 42)
model.fit(X_train, y_train)

# Pr√©dictions
y_pred = model.fit(X_train, y_train).predict(X_test)

# √âvaluation
print("Rapport de Classification :")
print(classification_report(y_test, y_pred))

plt.figure(figsize = (6, 5))
sns.heatmap(confusion_matrix(y_test, y_pred), annot = True, fmt = 'd', cmap = 'Blues')
plt.title('Matrice de Confusion')
plt.ylabel('Vrai label')
plt.xlabel('Label pr√©dit')
plt.show()`
                        }
                    ]
                },
                {
                    id: 'marketing',
                    title: 'Marketing & Client',
                    description: 'Segmentation et analyse comportementale.',
                    snippets: [
                        {
                            id: 'rfm_segmentation',
                            title: 'Segmentation RFM',
                            description: 'Segmenter les clients par R√©cence, Fr√©quence et Montant.',
                            markdown: `### üéØ Objectif de la Segmentation RFM

La segmentation **RFM** (Recency, Frequency, Monetary) est une m√©thode √©prouv√©e du marketing pour identifier et classer vos clients en fonction de leur comportement d'achat.

**Pourquoi utiliser RFM ?**
- üìä **Identifier vos meilleurs clients** : Qui g√©n√®re le plus de valeur ?
- üéØ **Cibler vos actions marketing** : Personnaliser vos campagnes selon le segment
- üí∞ **Optimiser le ROI** : Concentrer vos efforts l√† o√π ils rapportent le plus

**Les 3 Dimensions :**
1. **R√©cence (R)** : Depuis combien de temps le client n'a pas achet√© ? (Plus c'est r√©cent, mieux c'est)
2. **Fr√©quence (F)** : Combien de fois le client a achet√© ? (Plus il ach√®te, mieux c'est)
3. **Montant (M)** : Combien le client d√©pense au total ? (Plus il d√©pense, mieux c'est)

**La Logique de Scoring :**
Chaque dimension est not√©e de **1 √† 5** (5 = meilleur). Un client not√© **555** est un **Champion** üèÜ (r√©cent, fr√©quent, gros montant), tandis qu'un client **111** est **√† risque** ‚ö†Ô∏è.`,
                            code: `import pandas as pd
import numpy as np
import datetime as dt

# --- 1. G√©n√©ration de Donn√©es de Vente ---
np.random.seed(42)
n_transactions = 1000
dates = pd.date_range(end=dt.datetime.today(), periods=365).to_list()

df = pd.DataFrame({
    'transaction_id': range(n_transactions),
    'customer_id': np.random.randint(1, 200, size=n_transactions), # 200 clients uniques
    'date': np.random.choice(dates, size=n_transactions),
    'amount': np.random.exponential(scale=50, size=n_transactions).round(2) + 10 # Montant > 10‚Ç¨
})

# --- 2. Calcul RFM (Agr√©gation par Client) ---
# On d√©finit "maintenant" comme le jour apr√®s la derni√®re transaction
now = df['date'].max() + dt.timedelta(days=1)

rfm = df.groupby('customer_id').agg({
    'date': lambda x: (now - x.max()).days,  # R (Recency) : Nombre de jours depuis le dernier achat
    'transaction_id': 'count',                # F (Frequency) : Nombre d'achats total
    'amount': 'sum'                           # M (Monetary) : Somme totale d√©pens√©e
}).rename(columns={'date': 'R', 'transaction_id': 'F', 'amount': 'M'})

# --- 3. Scoring par Quintiles (Division en 5 groupes) ---
# Chaque client re√ßoit un score de 1 √† 5 pour chaque dimension

# R_Score : ATTENTION, pour la R√©cence, plus le nombre de jours est PETIT, mieux c'est
# Donc on inverse : un client qui a achet√© r√©cemment (R petit) aura un score de 5
rfm['R_Score'] = pd.qcut(rfm['R'], 5, labels=[5, 4, 3, 2, 1])

# F_Score : Plus le client ach√®te souvent, meilleur est le score (1 √† 5)
# rank(method='first') √©vite les erreurs si plusieurs clients ont la m√™me fr√©quence
rfm['F_Score'] = pd.qcut(rfm['F'].rank(method='first'), 5, labels=[1, 2, 3, 4, 5])

# M_Score : Plus le client d√©pense, meilleur est le score (1 √† 5)
rfm['M_Score'] = pd.qcut(rfm['M'], 5, labels=[1, 2, 3, 4, 5])

# --- 4. Cr√©ation du Segment RFM (Concat√©nation des Scores) ---
# Exemple : Un client avec R=5, F=5, M=4 aura le segment "554"
rfm['RFM_Segment'] = rfm['R_Score'].astype(str) + rfm['F_Score'].astype(str) + rfm['M_Score'].astype(str)

# Score_Total : Somme des 3 scores (de 3 √† 15)
# Utilis√© pour classer facilement les clients
rfm['Score_Total'] = rfm[['R_Score', 'F_Score', 'M_Score']].sum(axis=1)

# --- 5. Segmentation Business (Labels Parlants) ---
def segment_customer(score):
    """
    Transforme le score num√©rique en label business actionnable
    
    - Champions (13-15) : Vos meilleurs clients. R√©compensez-les (programme VIP).
    - Fid√®les (10-12) : Clients r√©guliers. Encouragez-les √† devenir Champions.
    - √Ä R√©veiller (7-9) : Inactifs mais potentiel. Relancez avec une offre cibl√©e.
    - √Ä Risque (3-6) : En perte de vitesse. Action urgente avant qu'ils partent.
    """
    if score >= 13: return 'üèÜ Champions'
    elif score >= 10: return 'üíé Fid√®les'
    elif score >= 7:  return 'üí§ √Ä R√©veiller'
    else:             return '‚ö†Ô∏è √Ä Risque'

rfm['Segment_Label'] = rfm['Score_Total'].apply(segment_customer)

# --- 6. Affichage des R√©sultats ---
print(rfm[['R', 'F', 'M', 'R_Score', 'F_Score', 'M_Score', 'Score_Total', 'Segment_Label']].head(10))
print("\\n--- Distribution des Segments ---")
print(rfm['Segment_Label'].value_counts())`
                        }
                    ]
                },
                {
                    id: 'production_ml',
                    title: 'Mise en Production (MLOps)',
                    description: 'Pipelines robustes et Transformers personnalis√©s.',
                    snippets: [
                        {
                            id: 'sklearn_custom_pipeline',
                            title: 'Pipeline Sklearn Custom',
                            description: 'Cr√©er un Transformer personnalis√© pour nettoyer et enrichir les donn√©es.',
                            markdown: `### üîß Objectif : Pipeline de Preprocessing Robuste

Un **Pipeline Scikit-Learn** permet d'encha√Æner plusieurs √©tapes de transformation de donn√©es de mani√®re **automatique**, **reproductible** et **d√©ployable** en production.

**Pourquoi cr√©er des Transformers personnalis√©s ?**
- üßπ **Nettoyage m√©tier** : Standardiser les donn√©es textuelles (casse, espaces, valeurs manquantes)
- üöÄ **Feature Engineering** : Cr√©er des variables calcul√©es (ex: prix au m¬≤)
- üîÑ **R√©utilisabilit√©** : Appliquer les m√™mes transformations sur Train ET Test (√©vite le Data Leakage)
- üì¶ **Production** : Sauvegarder le pipeline complet avec \`joblib\` ou \`pickle\`

**Architecture d'un Transformer Custom :**
1. H√©riter de \`BaseEstimator\` et \`TransformerMixin\`
2. Impl√©menter \`fit()\` : Apprendre des statistiques (si n√©cessaire)
3. Impl√©menter \`transform()\` : Appliquer les transformations

**Avantages du Pipeline :**
- ‚úÖ Pas de risque d'oublier une √©tape sur les nouvelles donn√©es
- ‚úÖ Code propre et maintenable
- ‚úÖ Compatible avec GridSearchCV pour le tuning des hyperparam√®tres`,
                            code: `import pandas as pd
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

# --- Donn√©es Exemple : Catalogue Immobilier ---
# Probl√®mes typiques : Texte sale, valeurs manquantes, unit√©s incoh√©rentes
df = pd.DataFrame({
    'description': [' Produit A ', 'produit B', 'PRODUIT A', None, 'Produit C'],
    'prix': [100, 200, 100, 50, None],  # Prix en milliers d'euros
    'surface': [50, 60, 50, 100, 20]     # Surface en m¬≤
})

# --- Transformer Personnalis√© 1 : Nettoyage de Texte ---
class TextCleaner(BaseEstimator, TransformerMixin):
    """
    Nettoie une colonne texte en :
    1. Rempla√ßant les NaN par 'inconnu'
    2. Supprimant les espaces superflus
    3. Normalisant la casse (minuscule ou majuscule)
    
    Param√®tres :
    - column (str) : Nom de la colonne √† nettoyer
    - case (str) : 'lower' (d√©faut) ou 'upper' pour la normalisation
    """
    def __init__(self, column, case='lower'):
        self.column = column
        self.case = case
    
    def fit(self, X, y=None):
        # Ce transformer ne "apprend" rien des donn√©es, il applique juste des r√®gles
        return self
    
    def transform(self, X):
        X_copy = X.copy()  # IMPORTANT : Toujours copier pour ne pas modifier l'original
        
        # 1. Gestion des valeurs manquantes (NaN -> 'inconnu')
        X_copy[self.column] = X_copy[self.column].fillna('inconnu')
        
        # 2. Suppression des espaces en d√©but/fin de cha√Æne
        X_copy[self.column] = X_copy[self.column].str.strip()
        
        # 3. Normalisation de la casse (ex: "Produit A" -> "produit a")
        if self.case == 'lower':
            X_copy[self.column] = X_copy[self.column].str.lower()
        elif self.case == 'upper':
            X_copy[self.column] = X_copy[self.column].str.upper()
        
        return X_copy

# --- Transformer Personnalis√© 2 : Feature Engineering ---
class PricePerSqm(BaseEstimator, TransformerMixin):
    """
    Cr√©e une nouvelle colonne 'prix_m2' (Prix au m√®tre carr√©).
    M√©tier : Indicateur cl√© en immobilier pour comparer les biens.
    
    G√®re automatiquement :
    - Division par z√©ro (si surface = 0, on consid√®re 1 pour √©viter l'erreur)
    """
    def fit(self, X, y=None):
        return self  # Pas d'apprentissage n√©cessaire
    
    def transform(self, X):
        X_copy = X.copy()
        
        # Calcul du prix au m¬≤ avec gestion de la division par z√©ro
        # replace(0, 1) : Si surface = 0, on la remplace par 1 (√©vite division par 0)
        X_copy['prix_m2'] = X_copy['prix'] / X_copy['surface'].replace(0, 1)
        
        return X_copy

# --- Construction du Pipeline Complet ---
# L'ORDRE des √©tapes est CRUCIAL : chaque √©tape re√ßoit la sortie de la pr√©c√©dente

data_pipeline = Pipeline([
    # √âtape 1 : Nettoyage du texte (standardisation des descriptions)
    ('clean_text', TextCleaner(column='description', case='lower')),
    
    # √âtape 2 : Feature Engineering m√©tier (cr√©ation du prix au m¬≤)
    # Cette variable peut √™tre tr√®s pr√©dictive pour un mod√®le ML
    ('feature_eng', PricePerSqm()),
    
    # √âtape 3 (optionnel, comment√© ici) : Imputation des valeurs manquantes num√©riques
    # ('impute_num', SimpleImputer(strategy='median')),
    
    # √âtape 4 (optionnel) : Scaling pour les algorithmes sensibles (SVM, KNN...)
    # ('scaler', StandardScaler())
])

# --- Ex√©cution du Pipeline ---
# fit_transform() applique toutes les √©tapes s√©quentiellement
df_transformed = data_pipeline.fit_transform(df)

# --- R√©sultat ---
print("=== Donn√©es Transform√©es ===")
print(df_transformed)

# --- Utilisation en Production ---
# 1. Sauvegarder le pipeline avec joblib
# import joblib
# joblib.dump(data_pipeline, 'preprocessing_pipeline.pkl')

# 2. Charger et appliquer sur de nouvelles donn√©es
# pipeline_loaded = joblib.load('preprocessing_pipeline.pkl')
# new_data_transformed = pipeline_loaded.transform(new_data)`
                        }
                    ]
                }
            ]
        },
        {
            id: 'python_date',
            title: 'Dates (Python)',
            description: 'Manipulation de dates et s√©ries temporelles.',
            categories: [

                {
                    id: 'practical_cases',
                    title: 'Cas Pratiques',
                    description: 'Exemples concrets et avanc√©s.',
                    snippets: [
                        {
                            id: 'french_calendar',
                            title: 'Calendrier Fran√ßais (F√©ri√©s & Ponts)',
                            description: 'D√©tecter les jours f√©ri√©s, les ponts et les retours de vacances.',
                            code: `import pandas as pd
import holidays
from datetime import timedelta

# 1. Cr√©ation d'un jeu de donn√©es exemple
dates = pd.date_range(start='2025-01-01', end='2025-12-31', freq='D')
df = pd.DataFrame({'date': dates})

# 2. Ajout du nom du jour (en fran√ßais)
days_fr = {
    0: 'Lundi', 1: 'Mardi', 2: 'Mercredi', 3: 'Jeudi', 
    4: 'Vendredi', 5: 'Samedi', 6: 'Dimanche'
}
df['jour_nom'] = df['date'].dt.dayofweek.map(days_fr)

# 3. Jours F√©ri√©s (France)
# N√©cessite : pip install holidays
fr_holidays = holidays.France(years=[2025])
df['jour_ferie'] = df['date'].apply(lambda x: x in fr_holidays)

# 4. Veille et Lendemain de jour f√©ri√©
# Shift(-1) -> La valeur de demain vient ici (donc si demain est f√©ri√©, ici c'est veille)
df['veille_jour_ferie'] = df['jour_ferie'].shift(-1).fillna(False)
df['lendemain_jour_ferie'] = df['jour_ferie'].shift(1).fillna(False)

# 5. Jour Ouvr√© (Lundi-Vendredi ET Pas f√©ri√©)
df['jour_ouvre'] = (df['date'].dt.dayofweek < 5) & (~df['jour_ferie'])

# 6. Jour Ouvr√© Lendemain de F√©ri√© (Retour au travail)
# Logique : C'est un jour ouvr√©, et le jour pr√©c√©dent (ou la s√©quence de jours pr√©c√©dents) √©tait f√©ri√©/weekend.
def is_return_from_holiday(idx, df):
    if not df.loc[idx, 'jour_ouvre']:
        return False
    
    # On regarde en arri√®re
    prev_idx = idx - 1
    while prev_idx >= 0:
        if df.loc[prev_idx, 'jour_ouvre']:
            return False # On a trouv√© un jour ouvr√© avant, donc ce n'est pas un retour de vacances
        if df.loc[prev_idx, 'jour_ferie']:
            return True # On a trouv√© un f√©ri√© sans croiser de jour ouvr√© -> C'est un retour !
        prev_idx -= 1
        
    return False

df['jour_ouvre_lendemain_ferie'] = [is_return_from_holiday(i, df) for i in range(len(df))]

# Aper√ßu
print(df[['date', 'jour_nom', 'jour_ferie', 'jour_ouvre', 'jour_ouvre_lendemain_ferie']].head(15))`
                        },
                        {
                            id: 'school_holidays',
                            title: 'Vacances Scolaires (Avanc√©)',
                            description: 'R√©cup√©ration API, gestion des zones et d√©duplication robuste.',
                            code: `import pandas as pd
import numpy as np

# --- 1. CONFIGURATION & DONN√âES ---
URL_API = "https://data.education.gouv.fr/api/explore/v2.1/catalog/datasets/fr-en-calendrier-scolaire/exports/csv?lang=fr&timezone=Europe%2FParis&use_labels=true&delimiter=%3B"

DEPARTMENTS_ZONES = {
    # Zone A
    '01': 'Zone A', '03': 'Zone A', '07': 'Zone A', '15': 'Zone A', '16': 'Zone A', '17': 'Zone A', '19': 'Zone A', '21': 'Zone A', '23': 'Zone A', '24': 'Zone A', '25': 'Zone A', '26': 'Zone A', '33': 'Zone A', '38': 'Zone A', '39': 'Zone A', '40': 'Zone A', '42': 'Zone A', '47': 'Zone A', '58': 'Zone A', '63': 'Zone A', '64': 'Zone A', '69': 'Zone A', '70': 'Zone A', '71': 'Zone A', '73': 'Zone A', '74': 'Zone A', '79': 'Zone A', '86': 'Zone A', '87': 'Zone A', '90': 'Zone A',
    # Zone B
    '02': 'Zone B', '04': 'Zone B', '05': 'Zone B', '06': 'Zone B', '08': 'Zone B', '10': 'Zone B', '13': 'Zone B', '14': 'Zone B', '18': 'Zone B', '22': 'Zone B', '27': 'Zone B', '28': 'Zone B', '29': 'Zone B', '35': 'Zone B', '36': 'Zone B', '37': 'Zone B', '41': 'Zone B', '44': 'Zone B', '45': 'Zone B', '49': 'Zone B', '50': 'Zone B', '51': 'Zone B', '52': 'Zone B', '53': 'Zone B', '54': 'Zone B', '55': 'Zone B', '56': 'Zone B', '57': 'Zone B', '59': 'Zone B', '60': 'Zone B', '61': 'Zone B', '62': 'Zone B', '67': 'Zone B', '68': 'Zone B', '72': 'Zone B', '76': 'Zone B', '80': 'Zone B', '83': 'Zone B', '84': 'Zone B', '85': 'Zone B', '88': 'Zone B',
    # Zone C
    '09': 'Zone C', '11': 'Zone C', '12': 'Zone C', '30': 'Zone C', '31': 'Zone C', '32': 'Zone C', '34': 'Zone C', '46': 'Zone C', '48': 'Zone C', '65': 'Zone C', '66': 'Zone C', '75': 'Zone C', '77': 'Zone C', '78': 'Zone C', '81': 'Zone C', '82': 'Zone C', '91': 'Zone C', '92': 'Zone C', '93': 'Zone C', '94': 'Zone C', '95': 'Zone C',
    # DOM
    '971': 'Guadeloupe', '972': 'Martinique', '973': 'Guyane', '974': 'La R√©union', '976': 'Mayotte'
}

def preparer_calendrier_vacances():
    print("T√©l√©chargement et pr√©paration des vacances...")
    try:
        df = pd.read_csv(URL_API, sep=';')
    except Exception as e:
        print(f"Erreur API ({e}), utilisation de donn√©es factices pour l'exemple.")
        # Fallback pour l'exemple si pas d'internet
        data = {'Description': ['No√´l', 'No√´l'], 'Zones': ['Zone A', 'Zone C'], 
                'Date de d√©but': ['2023-12-23', '2023-12-23'], 'Date de fin': ['2024-01-08', '2024-01-08']}
        df = pd.DataFrame(data)

    # Nettoyage de base
    df = df[['Description', 'Zones', 'Date de d√©but', 'Date de fin']].copy()
    df['start'] = pd.to_datetime(df['Date de d√©but'], utc=True).dt.date
    df['end'] = pd.to_datetime(df['Date de fin'], utc=True).dt.date
    
    # Filtrage des zones
    df = df[df['Zones'].isin(['Zone A', 'Zone B', 'Zone C'])]
    
    # √âtape 1 : On explose les p√©riodes en jours individuels
    holiday_days = []
    for _, row in df.iterrows():
        # end est exclusif dans date_range, mais inclusif dans les donn√©es √©ducation ? 
        # V√©rification standard : souvent [start, end[. Si end est le jour de reprise, il faut faire -1 jour.
        dates_in_holiday = pd.date_range(start=row['start'], end=row['end'] - pd.Timedelta(days=1))
        
        for d in dates_in_holiday:
            holiday_days.append({
                'date_ref': d.date(),
                'zone': row['Zones'],
                'vacances_nom': row['Description']
            })
            
    df_flat = pd.DataFrame(holiday_days)
    
    # --- FIX ANTI-DOUBLONS ---
    # C'est ici que la magie op√®re.
    # On regroupe par [date, zone]. Si doublon, on garde le nom unique ou on concat√®ne.
    # Ex: Si on a "Vacances Hiver" et "Vacances Hiver" -> on garde une seule fois.
    df_flat = df_flat.groupby(['date_ref', 'zone'], as_index=False).agg({
        'vacances_nom': lambda x: ' / '.join(sorted(set(str(v) for v in x if pd.notna(v))))
    })
    
    return df_flat

# --- 2. EX√âCUTION ---

# A. Pr√©paration du r√©f√©rentiel (unique par jour/zone)
df_calendrier_flat = preparer_calendrier_vacances()

# B. G√©n√©ration de Donn√©es Exemple
print("G√©n√©ration de donn√©es test...")
# df_user = pd.read_excel(r'C:\\Users\\bouss\\Downloads\\Test.xlsx', sheet_name='Feuil1')
# Pour l'exemple reproductible, on cr√©e un DataFrame √† la vol√©e :
df_user = pd.DataFrame({
    'date': ['2024-02-12', '2024-02-12', '2024-02-20'],
    'departement': ['75', '33', '33'] # 75=Paris(C), 33=Gironde(A)
})

# Sauvegarde du nombre de lignes pour v√©rification
nb_lignes_avant = len(df_user)

# C. Pr√©paration Utilisateur
df_user['date_ref'] = pd.to_datetime(df_user['date']).dt.date
# Nettoyage d√©partement (string, 2 chiffres)
df_user['departement'] = pd.to_numeric(df_user['departement'], errors='coerce').astype('Int64').astype(str).str.zfill(2)
df_user['zone'] = df_user['departement'].map(DEPARTMENTS_ZONES).fillna('Hors Zone')

# D. Fusion (Left Join)
df_final = pd.merge(
    df_user,
    df_calendrier_flat,
    on=['date_ref', 'zone'],
    how='left'
)

# E. Finalisation
df_final['en_vacances'] = df_final['vacances_nom'].notna()
df_final['vacances_nom'] = df_final['vacances_nom'].fillna('Non')
df_final = df_final.drop(columns=['date_ref'])

# V√âRIFICATION FINALE
nb_lignes_apres = len(df_final)
print("-" * 30)
if nb_lignes_avant == nb_lignes_apres:
    print(f"SUCC√àS : Le fichier contient bien {nb_lignes_apres} lignes (pas de doublons).")
else:
    print(f"ATTENTION : Le fichier est pass√© de {nb_lignes_avant} √† {nb_lignes_apres} lignes !")

print(df_final.head())`
                        }
                    ]
                }
            ]
        }
    ]
};
